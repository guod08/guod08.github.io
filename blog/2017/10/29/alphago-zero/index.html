
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>AlphaGo Zero浅读 - Dong Guo's Blog</title>
  <meta name="author" content="Dong Guo">

  
  <meta name="description" content="上周在滴滴内部分享了这篇文章读后感，这里记录一下浅见。 1.问题的本质和AlphaGo的解决思路 1.1 下围棋在计算机中是什么问题？ 这是有限空间的解搜索问题，问题的复杂性来自于搜索空间的巨大和难以穷尽。类似但更简单的的问题包括：走迷宫，下五子棋，以及各种策略游戏。这些问题很多已经被较好地解决（ &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://dongguo.me/blog/2017/10/29/alphago-zero">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/atom.xml" rel="alternate" title="Dong Guo's Blog" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <!--<script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>-->
  <script src="//libs.baidu.com/jquery/1.7.2/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="./javascripts/libs/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<!--<link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">-->
<!--<link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">-->
<script>
        function addBlankTargetForLinks () {
          $('a[href^="http"]').each(function(){
            $(this).attr('target', '_blank');
          });
        }
        $(document).bind('DOMNodeInserted', function(event) {
          addBlankTargetForLinks();
        });
</script>

  

</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">Dong Guo's Blog</a></h1>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:dongguo.me" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div>
<article class="hentry" role="article">
  
  <header>
    
      <h1 class="entry-title">AlphaGo Zero浅读</h1>
    
    
      <p class="meta">
        








  


<time datetime="2017-10-29T22:14:13+08:00" pubdate data-updated="true">Oct 29<span>th</span>, 2017</time>
        
        
           | <a href="#comments">Comments</a>
        
      </p>
    
  </header>


<div class="entry-content"><p>上周在滴滴内部分享了这篇文章读后感，这里记录一下浅见。</p>

<h2>1.问题的本质和AlphaGo的解决思路</h2>

<h3>1.1 下围棋在计算机中是什么问题？</h3>

<p>这是有限空间的解搜索问题，问题的复杂性来自于搜索空间的巨大和难以穷尽。类似但更简单的的问题包括：走迷宫，下五子棋，以及各种策略游戏。这些问题很多已经被较好地解决（超过人类专家的水准）。</p>

<p><img src="http://dongguo.me/images/personal/alphago/mcts_go.png" alt="" /></p>

<p>AlphaGo之所以获得这么大的反响，是由于这个问题大得多的空间复杂性以及之前对该问题的论断(预测几十年之后才能超越人类)。以及David Silver团队一牛的技术营销能力。</p>

<h3>1.2 AlphaGo的解法思路是直观的</h3>

<p>AlphaGo（包括Zero）解决下围棋问题的“思路”是比较朴素的，受过基础算法训练的同学应该能想到一些，当然难度在于有了思路之后解决的方式和程度。</p>

<ul>
<li>look forward（向前多看几步，人类下五子棋、象棋、围棋都是这个思路）</li>
<li>学习高手是如何落子的</li>
<li>评估某个棋局的赢面</li>
</ul>


<p>这也是AlphaGo的Methodology。</p>

<h2>2.AlphaGo(2016年)的解法</h2>

<h3>2.1 核心之一：MCTS</h3>

<p><img src="http://dongguo.me/images/personal/alphago/mcts.png" alt="" /></p>

<p>MCTS（Monto Carlo Tree Search）是go中的一个核心算法（在AlphaGo之前的各种围棋程序基本都基于MCTS）。MCTS核心思想是基于反复的episode sampling高效地建立一棵树状结构，高效体现在其重点exploit较优的path，且兼顾了exploration。树建立好之后，每个节点记录了Q-value信息，直接可用于Action决策。几个步骤如下：</p>

<ol>
<li>Selection：基于棋局当前的state定位到Tree的一个叶子节点，每个节点存储Q-value（以及prior probability）和visit count；Q-value的初始值通过Policy Network的prediction</li>
<li>Expansion：当叶节点visit count大于某个阈值，继续从深度上拓展该path，使用default policy做sampling；</li>
<li>Evaluation：combine Q-value with one random rollout result, and update value of state</li>
<li>Backup: 更新tree（update visit count，and Q-value）</li>
</ol>


<p>在AlphaGo之前，各种围棋程序的解法基本就是依赖MCTS及其变种，问题是go的解空间（包括深度和宽度）巨大，AlphaGo的主要改进是使用Policy Network作为MCTS的先验将树搜索宽度限制在了高概率的分支上（narrow down search to high-probability moves），以及使用Value Network减少了树的搜索深度（当Value Network预测叶节点的胜率比较极端，就没有必要做expansion或rollout了），将探索集中在高效的空间，从而显著提升了power。</p>

<h3>2.2 整体步骤</h3>

<p><img src="http://dongguo.me/images/personal/alphago/alphago_network_pipeline.png" alt="" /></p>

<ol>
<li><p>Policy Network的第一步训练：基于Human knowledge（专业棋手的落子），使用DNN，有监督学习人类的落子（Q-value），就是一个分类问题；
<img src="http://dongguo.me/images/personal/alphago/policy_sl.png" alt="" /></p></li>
<li><p>Policy Network的第二步训练：在第一步有监督学习到的Network参数基础上，使用self-play（不同的policy）进行对弈，基于Policy Gradient RL进一步优化Network；
<img src="http://dongguo.me/images/personal/alphago/policy_rl.png" alt="" /></p></li>
<li><p>Value Network训练：学习每个state的赢面，使用policy network一样的网络结构（除输出层），基于MC value-based RL进行训练
<img src="http://dongguo.me/images/personal/alphago/value_rl.png" alt="" /></p></li>
<li><p>将学习到的Policy Network作为MCTS节点的prior probability，指导子节点的选择。将学习到的Value Network作为MCTS节点state value的prior probability，结合rollout的结果，一起update MCTS各个节点的信息</p></li>
</ol>


<h2>3.AlphaGo Zero(2017年)</h2>

<h3>3.1 算法改进点</h3>

<p>AlphaGo Zero相比AlphaGo最大的不同是将MCTS的建立过程和Network的训练融合在一起，互相迭代（而不是像AlphaGo中先单独训练Network，再将Network灌到MCTS中）。第一，MCTS的Q-value和eposide self-play的胜负一起指导Network的训练（trust MCTS）；第二，MCTS的Q-value指导Network self-play的move；第三，Network的预估结果迭代地作为MCTS节点Q-value的prior probability（而不是第一版AlphaGo中一次性地加入）</p>

<ol>
<li>随机初始化Network（注意Zero中Policy和Value共享一个Network），初始化MCTS（各个node的信息）</li>
<li>如下几个步骤不断迭代</li>
<li>在当前棋局state下，基于MCTS的selection，指导Network self-play的move</li>
<li>基于上一轮的Network参数，指导MCTS做selection（update Q-value的prior probability）</li>
<li>一盘棋结束，基于eposide的胜负（state value），以及MCTS的Q-value，指导Network参数的迭代</li>
</ol>


<p>Loss function如下：
<img src="http://dongguo.me/images/personal/alphago/zero_lf.png" alt="" /></p>

<p>如上是我理解AlphaGo Zero在算法上的最大改进。其余几点改进如下（部分是由于前者间接带来的）：</p>

<ol>
<li>2个Network合成一个（这个我没有太多感受，一些文章提到通过2个network分别来建模policy和state-value是之前的best practice）</li>
<li>without any supervision of human data（不再依赖学习人类棋手的move做policy network的初始化）；这一点我理解是由算法改进带来，之前的版本在训练Policy Network时没有MCTS的协助，仅仅依靠policy network本身做self-policy确实肯定搞不定；</li>
<li>Network结构：使用了residual network，这一点提升了600 Elo；不同blocks数目power有一定差异</li>
<li>MCTS不再需要rollout：这一点也好理解，当MCTS的buiding和Network training融合，training过程从self-play中拿到了eposide胜负信息输入</li>
</ol>


<h3>3.2 效果对比</h3>

<p><img src="http://dongguo.me/images/personal/alphago/compare1.png" alt="" /></p>

<p>paper中这张图除了表明Zero的威力，还表达了Zero相比Supervised Learning得到的Policy并不能更好地预估Human expert的move，但是Elo却高很多，即：Zero学到的技艺和人类围棋知识不同，且人类大脑有限的计算能力很可能陷入了局部解。</p>

<p><img src="http://dongguo.me/images/personal/alphago/compare2.png" alt="" /></p>

<p>几个不同算法的performance差异，提示：2个player如果Elo差距400分，分高的胜率在90%。</p>

<h2>4.综合感受</h2>

<ol>
<li>RL在解决无即时反馈问题上的能力扩宽了想象力</li>
<li>不使用human expert move进行学习，这个我也觉得比较好理解，围棋本质就是有限空间的解搜索问题，在有强大计算能力和好的算法时，不依赖人的数据理论上是可行的（况且确实有被人类带入狭隘解空间的风险，本来就是要PK掉人类的）</li>
<li>适用性：围棋是一个非常有确定性的问题，一方面输赢的定义很明确，另一方面除了落子没有其他因素影响到棋局；而很多我们在滴滴遇到的问题并没有如此简单；</li>
<li>很多ML理论和实践的工作都用到了sampling，以及左右互搏迭代的思想，Alphago Zero也是。该思想值得在工作创新中重视</li>
<li>Engineering的重要性，不仅是集群和计算能力，更多的是通过所谓的tricks（其实大多是从数据中发现问题）解决仅有理论或idea搞不定的问题（DQN和alphaGo这3篇paper中deepmind体现的淋漓尽致）</li>
<li>从16年到17年，alphaGo的复杂性是降低了（算法过程、计算需求、网络），更简洁却更好地解决问题，这个是境界了</li>
<li>deepmind团队（或者说google）体现的强大技术营销能力，确实佩服！</li>
</ol>


<h2>5.几篇文章和code</h2>

<ol>
<li>2015, Human-level control through deep reinforcement learning: Beat human in some games without human knowledge</li>
<li>2016, Mastering the Game of Go with Deep Neural Networks and Tree Search: Beat human in Go game partially rely on human knowledge</li>
<li>2017, Mastering the Gmme of Go without Human knowledge: Dominate human in Go game without human knowledge</li>
<li>在知乎看到有Alphago Zero的开源<a href="https://github.com/gcp/leela-zero">实现版本</a>，未验证仅供参考</li>
</ol>

</div>


  <footer>
    <p class="meta">
      
  

<span class="byline author vcard">Posted by <span class="fn">Dong Guo</span></span>

      








  


<time datetime="2017-10-29T22:14:13+08:00" pubdate data-updated="true">Oct 29<span>th</span>, 2017</time>
      


    </p>
    
      <div class="sharing">
  
  
  
    <div class="fb-like" data-send="true" data-width="450" data-show-faces="false"></div>
  
</div>

    
    <p class="meta">
      
        <a class="basic-alignment left" href="/blog/2017/10/18/critical-thinking/" title="Previous Post: 读书感想-批判性思维(Critical_Thinking)">&laquo; 读书感想-批判性思维(Critical_Thinking)</a>
      
      
        <a class="basic-alignment right" href="/blog/2017/12/12/pricing-papers/" title="Next Post: 2017年秋-定价工作部分参考文献">2017年秋-定价工作部分参考文献 &raquo;</a>
      
    </p>
  </footer>
</article>


  <section>
    <h1>Comments</h1>
    <div id="comments" aria-live="polite"><!-- Duoshuo Comment BEGIN -->
<div class="ds-thread"></div>
<script type="text/javascript">
    var duoshuoQuery = {short_name:"guod08"};
  (function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = 'http://static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0] 
    || document.getElementsByTagName('body')[0]).appendChild(ds);
  })();
</script>
<!-- Duoshuo Comment END -->
</div>
  </section>

</div>

<aside class="sidebar">
  
    <section>
  <h1>Dong Guo</h1>
  <p>滴滴打车-算法专家 (策略设计|机器学习|优化方法). 
  <br></br>Previously at Hulu and Baidu. Graduated from Tsinghua University.
  <br></br>
  Email: guod08[at]gmail.com<br> 
  <a href="http://www.linkedin.com/in/dongguo1">Linkedin</a>, <a href="https://github.com/guod08/">Github</a>, <a href="http://www.slideshare.net/guo_dong">SlideShare</a> </p>
</section>
<section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2018/08/24/shu-ping-mei-tuan-ji-qi-xue-xi-shi-jian/">书评-美团机器学习实践</a>
      </li>
    
      <li class="post">
        <a href="/blog/2018/07/22/okr/">OKR工作法</a>
      </li>
    
      <li class="post">
        <a href="/blog/2018/06/22/slides-about-reinforcement-learning/">Slides About Reinforcement Learning</a>
      </li>
    
      <li class="post">
        <a href="/blog/2018/02/16/career2017/">Principles-2017工作部分心得</a>
      </li>
    
      <li class="post">
        <a href="/blog/2017/12/12/pricing-papers/">2017年秋-定价工作部分参考文献</a>
      </li>
    
      <li class="post">
        <a href="/blog/2017/10/29/alphago-zero/">AlphaGo Zero浅读</a>
      </li>
    
      <li class="post">
        <a href="/blog/2017/10/18/critical-thinking/">读书感想-批判性思维(Critical_Thinking)</a>
      </li>
    
      <li class="post">
        <a href="/blog/2017/10/07/xinjiang-travel/">2017国庆-北疆自驾</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/12/18/experiment/">数据驱动的核心：Controlled Experiments</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/12/06/archsummit2016/">ArchSummit2016干货分享</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/07/03/future-transportation/">出行的未来</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/02/13/ri-ji-2016-02-13-semi-supervised-learning-Reinforcement-learning/">日记2016/02/13:周志华老师的新书</a>
      </li>
    
      <li class="post">
        <a href="/blog/2015/07/25/didi-strategy-team-welcome-you/">滴滴出行-大数据策略组请你加入</a>
      </li>
    
      <li class="post">
        <a href="/blog/2015/07/18/wo-zai-di-di-yu-dao-de-ji-zhu-tiao-zhan/">我在滴滴遇到的技术挑战</a>
      </li>
    
      <li class="post">
        <a href="/blog/2015/03/28/shi-yong-shadowsockske-xue-shang-wang/">使用Shadowsocks科学上网</a>
      </li>
    
      <li class="post">
        <a href="/blog/2015/03/24/what-i-learned-in-my-first-job/">我在第一份工作中学到了什么</a>
      </li>
    
      <li class="post">
        <a href="/blog/2015/03/02/druid-cluster-setup/">Druid Cluster Setup</a>
      </li>
    
      <li class="post">
        <a href="/blog/2015/02/12/cookies-matching-introduction/">在线广告中的cookie Matching</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/12/30/Spark-Usage-Share/">Spark使用经验分享</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/12/16/Machine-Learning-talks-from-me/">Slides of Some Machine Learning Talk I Shared in Hulu</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/11/28/programmatic-digital-advertising/">Programmatic Digital Advertising</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/09/20/ad-inventory-forecasting/">Challenges for a Word-class Ad Inventory Forecasting System</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/05/05/druid-introduction-and-practise/">Druid介绍与实践</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/01/01/expectation-propagation/">Expectation Propagation: Theory and Application</a>
      </li>
    
      <li class="post">
        <a href="/blog/2013/12/15/classification-models/">Classification Models</a>
      </li>
    
      <li class="post">
        <a href="/blog/2013/12/02/gaussian-and-truncated-gaussian/">Gaussian and Truncated Gaussian</a>
      </li>
    
      <li class="post">
        <a href="/blog/2013/12/01/bayesian-ctr-prediction-for-bing/">Bayesian CTR Prediction of Bing</a>
      </li>
    
      <li class="post">
        <a href="/blog/2013/11/12/distributed-system-theory/">Notes for Distributed System Theory</a>
      </li>
    
      <li class="post">
        <a href="/blog/2013/10/20/shi-jian-guan-li/">更高效地工作学习</a>
      </li>
    
  </ul>
</section>
<section>
  <h1>Tag Cloud</h1>
  <span id="tag-cloud"><a href='/blog/categories/bayesian' style='font-size: 120.0%'>bayesian(2)</a> <a href='/blog/categories/big-data' style='font-size: 140.0%'>big_data(4)</a> <a href='/blog/categories/career' style='font-size: 120.0%'>career(2)</a> <a href='/blog/categories/computing-ads' style='font-size: 130.0%'>computing_ads(3)</a> <a href='/blog/categories/data-platform' style='font-size: 120.0%'>data_platform(2)</a> <a href='/blog/categories/engineering' style='font-size: 160.0%'>engineering(6)</a> <a href='/blog/categories/machine-learning' style='font-size: 150.0%'>machine_learning(5)</a> <a href='/blog/categories/math' style='font-size: 110.0%'>math(1)</a> <a href='/blog/categories/spark' style='font-size: 110.0%'>spark(1)</a> </span>
</section>

<section>
  <h1>访客统计</h1>
    <br/>
    <a href="http://s11.flagcounter.com/more/PbH"><img src="http://s11.flagcounter.com/count2/PbH/bg_FFFFFF/txt_000000/border_CCCCCC/columns_2/maxflags_10/viewers_0/labels_0/pageviews_0/flags_0/percent_0/" alt="Flag Counter" border="0"></a>
</section>    

  
</aside>


    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2018 - Dong Guo -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  


<!--
<div id="fb-root"></div>
<script>(function(d, s, id) {
  var js, fjs = d.getElementsByTagName(s)[0];
  if (d.getElementById(id)) {return;}
  js = d.createElement(s); js.id = id; js.async = true;
  js.src = "//connect.facebook.net/en_US/all.js#appId=212934732101925&xfbml=1";
  fjs.parentNode.insertBefore(js, fjs);
}(document, 'script', 'facebook-jssdk'));</script>

-->
<!--
-->
<!--
-->



</body>
</html>
