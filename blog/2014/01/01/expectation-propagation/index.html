
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Expectation Propagation: Theory and Application - Dong Guo's Blog</title>
  <meta name="author" content="Dong Guo">

  
  <meta name="description" content="简介 第一次接触EP是10年在百度实习时，当时组里面正有计划把线上的CTR预估模型改成支持增量更新的版本，读到了微软一篇基于baysian的CTR预估模型的文章（见推荐阅读5），文章中没有给出推导的细节，自己也没有继续研究。今年在PRML中读Approximal &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://dongguo.me/blog/2014/01/01/expectation-propagation">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/atom.xml" rel="alternate" title="Dong Guo's Blog" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <!--<script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>-->
  <script src="//libs.baidu.com/jquery/1.7.2/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="./javascripts/libs/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<!--<link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">-->
<!--<link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">-->
<script>
        function addBlankTargetForLinks () {
          $('a[href^="http"]').each(function(){
            $(this).attr('target', '_blank');
          });
        }
        $(document).bind('DOMNodeInserted', function(event) {
          addBlankTargetForLinks();
        });
</script>

  

</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">Dong Guo's Blog</a></h1>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:dongguo.me" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div>
<article class="hentry" role="article">
  
  <header>
    
      <h1 class="entry-title">Expectation Propagation: Theory and Application</h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-01-01T22:04:00+08:00" pubdate data-updated="true">Jan 1<span>st</span>, 2014</time>
        
        
           | <a href="#comments">Comments</a>
        
      </p>
    
  </header>


<div class="entry-content"><h2>简介</h2>

<p>第一次接触EP是10年在百度实习时，当时组里面正有计划把线上的CTR预估模型改成支持增量更新的版本，读到了微软一篇基于baysian的CTR预估模型的文章（见推荐阅读5），文章中没有给出推导的细节，自己也没有继续研究。今年在PRML中读Approximal inference这章对EP有了一些了解，同时参考了其它相关的一些资料，在这里和大家探讨。</p>

<h4>什么是期望传播</h4>

<p>期望传播(Expectation Propagation): 基于<strong>bayesian</strong>的一种<strong>近似</strong>推断方法，常用于图模型中计算单个节点的边缘分布或者后验分布，属于message passing这一类推断方法。</p>

<h4>牛人</h4>

<p>首先当然是Thomas Minka, 其在MIT读博期间提出了EP，并将EP作为博士论文课题在2001年发表。Minka毕业之后去了CMU教书，现在和Bishop一起在剑桥微软研究院。</p>

<p>其次是Kevin p. Murphy, 他是我做EP相关文献调研时发现的paper比较多的，我读到的一篇全文基本都是在推导Minka博士论文中一些公式的细节。btw Murphy 2013年出版了一本书，见推荐阅读2。</p>

<h4>中英文对照</h4>

<p>下面是一些关键词的中英文对应 （由于相关的书籍文献基本都是英文的，有些词没有想到比较好的中文翻译，故保留英文）</p>

<p>截断高斯: Truncated Gaussian</p>

<p>置信传播: Belief Propagation （后面会简称BP）</p>

<p>期望传播: Expectation Propagation (后面会简称为EP)</p>

<p>消息传递: Message passing</p>

<h2>背景</h2>

<p>EP本身的思想和方法都还是比较简单的，不过会涉及到一些背景知识，这边一并介绍。</p>

<h3>高斯、截断高斯</h3>

<p>EP的核心思想之一是用指数族分布近似复杂分布，实际应用中通常选择高斯分布，所以多个高斯分布的乘积，相除，积分在EP应用过程中不可避免。</p>

<p>截断高斯是高斯分布在指定区间归一化后的结果，（所以其并不是一个高斯分布），EP本身并不和截断高斯直接相关，但是如果在分类问题中应用EP，对观察样本（0-1）建模方法通常是y=sign(f(x)>t), 和另一个高斯分布相乘之后即为截断高斯分布。（然后就需要计算其的均值方差，原因后面会提到）</p>

<p>我在另一篇文章<a href="http://dongguo.me/blog/2013/12/02/gaussian-and-truncated-gaussian/">Gaussian and Truncated Gaussian</a>中介绍了比较多的细节，可以参考。</p>

<h3>指数族分布</h3>

<p>指数族分布（exponential family distribution）有着非常好的特性，比如其有<strong>充分统计量</strong>，多个指数族分布的乘积依然是指数族分布，具体的介绍可以参见wikipedia, 介绍的非常全面，也可以参考PRML第2章。</p>

<p>由于指数族的良好特性，其常被拿去近似复杂的概率分布（EP和variance baysian都是）。由于EP中常常选择高斯分布，我们这边强调一下，高斯分布的充分统计量为: (x, x<sup>2</sup>), 其中x为高斯分布的自变量。</p>

<h3>图模型</h3>

<p>EP是贝叶斯流派的计算变量后验分布（或者说是边缘分布）的近似推断方法，通常都可以通过一个概率图模型来描述问题的生成过程（generation process），所以可以说图模型是EP的典型应用场景。</p>

<p>图模型在很多地方都有介绍，比如PRML第8章，在这里就不重复了。有1点提一下，一个图模型的联合分布（不管是有向图还是无向图）可以写成若干个因子的乘积，对于有向图每个因子是每个节点的条件分布（条件于其的所有直接相连的父节点），对于无限图每个因子是energy function。
这个特性在后面的置信传播算法会用到。</p>

<h3>factor graph</h3>

<p>图模型中节点之间的关系通过边来表达，factor graph将这种节点之间的关系通过显式的节点（factor node）来表达，比如对于有向图，每个factor node就代表一个条件概率分布，图中的所有的信息都存在于节点上（variable nodes和factor nodes）。</p>

<p>后面的BP和EP都基于factor graph，可以认为factor graph使得图上的inference方法变得比较直观，另一个好处是factor graph屏蔽了有向图和无向图的差异。（有向图无向图都可以转变为factor graph）</p>

<p>更多了解可以看PRML第8章。</p>

<h3>置信传播</h3>

<p>Belief Propagation (BP)又叫&#8217;sum-product&#8217;，是一种计算图模型上节点边缘分布的推断方法，属于消息传递方法的一种，非近似方法（基于其延伸的Loopy Belief propagation为近似推断方法）。
BP的核心为如下3点：</p>

<ul>
<li><strong>单个variable node边缘分布的计算</strong></li>
</ul>


<p><img src="/images/personal/research/ep-intro/marginal_dis_variable_node.png" width=400 align=top /></p>

<p>(注：上图来之PRML)</p>

<p>前面提到过图模型的联合分布可以分解为若干因子的乘积，每个因子对应一个factor node：</p>

<p><img src="/images/personal/research/ep-intro/posterior_dis_variable_node_f1.png" width=500 align=top /></p>

<p>每个variable node的边缘分布为与其直接相连的factor nodes传递过来的message的乘积：</p>

<p><img src="/images/personal/research/ep-intro/posterior_dis_variable_node_f2.png" width=700 align=top /></p>

<ul>
<li><strong>从factor node到variable node的消息传递</strong></li>
</ul>


<p><img src="/images/personal/research/ep-intro/message_factor_to_variable.png" width=400 align=bottom /></p>

<p>(注：上图来之PRML)</p>

<p>从factor node f传递到variable node x的message为：与f直接相连（除了x）的variable nodes传递到f的messages与f本身的乘积的积分（积分变量为与f直接相连的除x之外的所有variable nodes）：</p>

<p><img src="/images/personal/research/ep-intro/message_factor_to_variable_f1.png" width=600 align=top /></p>

<ul>
<li><strong>从variable node到factor node的消息传递</strong></li>
</ul>


<p><img src="/images/personal/research/ep-intro/message_variable_to_factor.png" width=400 align=bottom /></p>

<p>(注：上图来之PRML)</p>

<p>从variable node x到factor node f的message为：与x直接相连的factor nodes（除f以外）传递到x的messages的乘积：</p>

<p><img src="/images/personal/research/ep-intro/message_variable_to_factor_f1.png" width=400 align=top /></p>

<p>更多细节请参考PRML</p>

<h3>Moment matching</h3>

<p>在实际的问题中，要么后验分布本身比较复杂（推荐阅读3中的Clutter example），要么最大化后验的计算比较复杂，要么破坏了具体算法的假设（比如EP要求图中的所有message都是指数族），所以常常会用（有良好性质的）指数族分布近似实际的概率分布。</p>

<p><img src="/images/personal/research/ep-intro/moment_matching_1.png" width=600 align=top />
<img src="/images/personal/research/ep-intro/moment_matching_2.png" width=300 align=top /></p>

<p>用一个分布去近似另一个分布的常见方法是最小化KL散度:</p>

<p><img src="/images/personal/research/ep-intro/moment_matching_3.png" width=600 align=top />
<img src="/images/personal/research/ep-intro/moment_matching_4.png" width=600 align=top /></p>

<p>我们发现通过最小化KL散度得到的‘最接近’p(x)的q(x)可以简单地通过匹配充分统计量的期望得到。</p>

<p>当q(x)为高斯分布的时候，我们知道其充分统计量u(x)=(x, x<sup>2</sup>)，这时通过KL散度最小化近似分布近似的方法称为moment matching(匹配矩)</p>

<p><img src="/images/personal/research/ep-intro/moment_matching_5.png" width=600 align=top /></p>

<p>为什么称为匹配矩呢，看看矩的定义就知道了：</p>

<p><img src="/images/personal/research/ep-intro/moment_matching_6.png" width=350 align=top /></p>

<h2>期望传播方法-理论</h2>

<p>EP的思想：在图模型中，用高斯分布近似每一个factors，然后&#8217;approximate each factor in turn in the context of all remaining facotrs&#8217;.</p>

<p>下面为具体的算法：
<img src="/images/personal/research/ep-intro/ep_def.png" width=800 align=top />
(注：本算法参考了PRML)</p>

<p>下面通过Minka博士论文中的例子‘clutter problem’来解释：每个观察样本以(1-w)的概率由高斯分布N(x|sita, I)生成，以w的概率由noise生成（同样也是高斯分布N(x|0, aI)），于是：
<img src="/images/personal/research/ep-intro/clutter_problem_1.png" width=400 align=top />
<img src="/images/personal/research/ep-intro/clutter_problem_2.png" width=200 align=top /></p>

<p>按照EP的思想，我们用一个单高斯q(sita)去近似混合高斯p(x|sita)
<img src="/images/personal/research/ep-intro/clutter_problem_3.png" width=400 align=top /></p>

<p>单高斯去近似混合高斯听起来效果一定不好，但实际上，由于EP在近似的时候乘了其他所有factors的高斯近似之后的上下文，考虑到很多个高斯分布相乘之后的方差一般都很小，所有实际上单高斯只需要在很小的区间近似好混合高斯即可。如下图：</p>

<p><img src="/images/personal/research/ep-intro/clutter_problem_4.png" width=350 align=top />
<img src="/images/personal/research/ep-intro/clutter_problem_5.png" width=350 align=top /></p>

<p>(注：上面2张图来之PRML)</p>

<p>其中蓝色曲线为混合高斯（没有画完整），红色曲线为近似的单高斯，绿色曲线为‘其它所有factor的乘积’。</p>

<h3>EP怎么应用在message passing中：</h3>

<p>在图模型中，所谓的&#8217;context of all remaining factors&#8217;就是当前节点之外所有节点和messages，所以EP在图模型中的使用方式为：和BP一样的方法计算message和marginal distribution，当某个factor或者marginal distribution不是高斯分布时，用高斯分布近似它。所以Minka认为EP也就是BP+moment matching。</p>

<p>由于每个factor以及variable node的边缘分布都是高斯分布（或被近似为高斯分布），所以EP的计算过程一般并不复杂。</p>

<h2>期望传播方法-应用</h2>

<p>EP被广泛地应用在图模型的inference上，这边提一下微软的2个应用：Bing的CTR预估，XBOX游戏中player skill的评估。</p>

<h3>Bing的CTR预估</h3>

<p>详细的推导及实验请参考：<a href="http://dongguo.me/blog/2013/12/01/bayesian-ctr-prediction-for-bing/">Bayesian CTR prediction for Bing</a>
paper中称这个model为ad predictor，其在我的数据集上预估效果很不错，训练预测速度快，天然支持增量更新，主要的缺点就是模型不是稀疏的。如果你知道怎么自然地达到稀疏效果，请指教。</p>

<p>和其它算法的比较请参考：<a href="http://dongguo.me/blog/2013/12/15/classification-models/">Classification Models</a></p>

<h3>XBOX中player skill的评估</h3>

<p>图模型和上一篇略有差异，推导过程差不多，paper中没有给出详细的推导过程，不过Murphy的新书中给出了，请参考推荐阅读2。</p>

<h2>一些小结</h2>

<ol>
<li>EP的通用性比较好，对于实际的问题，画出graph model和factor graph，就可以尝试用EP来进行inference；</li>
<li>虽然应用EP时的推导过程略长（计算很多个message和marginal distribution），但是最终的整体的更新公式一般都非常简单，所以模型训练时间开销往往较小；</li>
<li>为了使用EP，只能用高斯分布来建模，比如Bing的CTR预估那篇对每个feature的weight建模，只能假设服从高斯分布，相当于是2范数的正则化，不能达到稀疏模型的效果；</li>
<li>在我的实验中，通过EP进行inference得到的模型预估效果不错，值得一试；</li>
</ol>


<h2>推荐阅读</h2>

<ol>
<li><p>机器学习保留书籍:<a href="http://research.microsoft.com/en-us/um/people/cmbishop/prml/">Pattern recognition and machine learning</a> 第2,8,10章 (第2章看看高斯四则运算，指数族分布特性；第8章了解图模型基础，期望传播算法；第10章了解期望传播算法)</p></li>
<li><p>Murphy新书: <a href="http://www.cs.ubc.ca/~murphyk/MLbook/">Machine Learning: A Probabilistic Perspective</a> 第22章 (本书相比PRML更加具体，第22章干脆包含了TrueSkill的详细推导步骤)</p></li>
<li><p>Minka的博士论文：<a href="http://qh.eng.ua.edu/e_paper/e_thesis/EPThesis.pdf">A family of algorithms for approximate Bayesian inference</a> (想了解基本思想和理论看完前3节即可)</p></li>
<li><p>EP的应用之一：<a href="http://research.microsoft.com/pubs/74419/tr-2006-80.pdf">TrueSkill: A Bayesian Skill Rating System</a> (文中并没有给出EP每一步的细节)</p></li>
<li><p>EP的应用之二：<a href="http://research.microsoft.com/pubs/122779/adpredictor%20icml%202010%20-%20final.pdf">Web-Scale Bayesian Click-Through Rate Prediction for Sponsored Search Advertising in Microsoft’s Bing Search Engine</a> (CTR预估的应用比较吸引人，文章写得很棒，算法的效果也很好，只是干脆忽略的inference过程，有兴趣的同学可以参看我另一个文章，里面有一步一步推导的过程)</p></li>
<li><p>Minka整理的EP学习资料：<a href="http://research.microsoft.com/en-us/um/people/minka/papers/ep/roadmap.html">link</a> (其中的包含了一个videolecture上他做的variance inference的talk值得一看)</p></li>
<li><p>本文的PPT： <a href="http://www.slideshare.net/guo_dong/expectation-propagation-researchworkshop">墙外</a>, <a href="http://pan.baidu.com/s/1kT3DtvL">墙内</a></p></li>
</ol>

</div>


  <footer>
    <p class="meta">
      
  

<span class="byline author vcard">Posted by <span class="fn">Dong Guo</span></span>

      








  


<time datetime="2014-01-01T22:04:00+08:00" pubdate data-updated="true">Jan 1<span>st</span>, 2014</time>
      

<span class="categories">
  
    <a class='category' href='/blog/categories/bayesian/'>bayesian</a>, <a class='category' href='/blog/categories/machine-learning/'>machine_learning</a>
  
</span>


    </p>
    
      <div class="sharing">
  
  
  
    <div class="fb-like" data-send="true" data-width="450" data-show-faces="false"></div>
  
</div>

    
    <p class="meta">
      
        <a class="basic-alignment left" href="/blog/2013/12/15/classification-models/" title="Previous Post: classification models">&laquo; classification models</a>
      
      
        <a class="basic-alignment right" href="/blog/2014/05/05/druid-introduction-and-practise/" title="Next Post: Druid介绍与实践">Druid介绍与实践 &raquo;</a>
      
    </p>
  </footer>
</article>


  <section>
    <h1>Comments</h1>
    <div id="comments" aria-live="polite"><!-- Duoshuo Comment BEGIN -->
<div class="ds-thread"></div>
<script type="text/javascript">
    var duoshuoQuery = {short_name:"guod08"};
  (function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = 'http://static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0] 
    || document.getElementsByTagName('body')[0]).appendChild(ds);
  })();
</script>
<!-- Duoshuo Comment END -->
</div>
  </section>

</div>

<aside class="sidebar">
  
    <section>
  <h1>Dong Guo</h1>
  <p>滴滴打车-算法专家 (策略设计|机器学习|优化方法). 
  <br></br>Previously at Hulu and Baidu. Graduated from Tsinghua University.
  <br></br>
  Email: guod08[at]gmail.com<br> 
  <a href="http://www.linkedin.com/in/dongguo1">Linkedin</a>, <a href="https://github.com/guod08/">Github</a>, <a href="http://www.slideshare.net/guo_dong">SlideShare</a> </p>
</section>
<section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2018/08/24/shu-ping-mei-tuan-ji-qi-xue-xi-shi-jian/">书评-美团机器学习实践</a>
      </li>
    
      <li class="post">
        <a href="/blog/2018/07/22/okr/">OKR工作法</a>
      </li>
    
      <li class="post">
        <a href="/blog/2018/06/22/slides-about-reinforcement-learning/">Slides About Reinforcement Learning</a>
      </li>
    
      <li class="post">
        <a href="/blog/2018/02/16/career2017/">Principles-2017工作部分心得</a>
      </li>
    
      <li class="post">
        <a href="/blog/2017/12/12/pricing-papers/">2017年秋-定价工作部分参考文献</a>
      </li>
    
      <li class="post">
        <a href="/blog/2017/10/29/alphago-zero/">AlphaGo Zero浅读</a>
      </li>
    
      <li class="post">
        <a href="/blog/2017/10/18/critical-thinking/">读书感想-批判性思维(Critical_Thinking)</a>
      </li>
    
      <li class="post">
        <a href="/blog/2017/10/07/xinjiang-travel/">2017国庆-北疆自驾</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/12/18/experiment/">数据驱动的核心：Controlled Experiments</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/12/06/archsummit2016/">ArchSummit2016干货分享</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/07/03/future-transportation/">出行的未来</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/02/13/ri-ji-2016-02-13-semi-supervised-learning-Reinforcement-learning/">日记2016/02/13:周志华老师的新书</a>
      </li>
    
      <li class="post">
        <a href="/blog/2015/07/25/didi-strategy-team-welcome-you/">滴滴出行-大数据策略组请你加入</a>
      </li>
    
      <li class="post">
        <a href="/blog/2015/07/18/wo-zai-di-di-yu-dao-de-ji-zhu-tiao-zhan/">我在滴滴遇到的技术挑战</a>
      </li>
    
      <li class="post">
        <a href="/blog/2015/03/28/shi-yong-shadowsockske-xue-shang-wang/">使用Shadowsocks科学上网</a>
      </li>
    
      <li class="post">
        <a href="/blog/2015/03/24/what-i-learned-in-my-first-job/">我在第一份工作中学到了什么</a>
      </li>
    
      <li class="post">
        <a href="/blog/2015/03/02/druid-cluster-setup/">Druid Cluster Setup</a>
      </li>
    
      <li class="post">
        <a href="/blog/2015/02/12/cookies-matching-introduction/">在线广告中的cookie Matching</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/12/30/Spark-Usage-Share/">Spark使用经验分享</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/12/16/Machine-Learning-talks-from-me/">Slides of Some Machine Learning Talk I Shared in Hulu</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/11/28/programmatic-digital-advertising/">Programmatic Digital Advertising</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/09/20/ad-inventory-forecasting/">Challenges for a Word-class Ad Inventory Forecasting System</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/05/05/druid-introduction-and-practise/">Druid介绍与实践</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/01/01/expectation-propagation/">Expectation Propagation: Theory and Application</a>
      </li>
    
      <li class="post">
        <a href="/blog/2013/12/15/classification-models/">Classification Models</a>
      </li>
    
      <li class="post">
        <a href="/blog/2013/12/02/gaussian-and-truncated-gaussian/">Gaussian and Truncated Gaussian</a>
      </li>
    
      <li class="post">
        <a href="/blog/2013/12/01/bayesian-ctr-prediction-for-bing/">Bayesian CTR Prediction of Bing</a>
      </li>
    
      <li class="post">
        <a href="/blog/2013/11/12/distributed-system-theory/">Notes for Distributed System Theory</a>
      </li>
    
      <li class="post">
        <a href="/blog/2013/10/20/shi-jian-guan-li/">更高效地工作学习</a>
      </li>
    
  </ul>
</section>
<section>
  <h1>Tag Cloud</h1>
  <span id="tag-cloud"><a href='/blog/categories/bayesian' style='font-size: 120.0%'>bayesian(2)</a> <a href='/blog/categories/big-data' style='font-size: 140.0%'>big_data(4)</a> <a href='/blog/categories/career' style='font-size: 120.0%'>career(2)</a> <a href='/blog/categories/computing-ads' style='font-size: 130.0%'>computing_ads(3)</a> <a href='/blog/categories/data-platform' style='font-size: 120.0%'>data_platform(2)</a> <a href='/blog/categories/engineering' style='font-size: 160.0%'>engineering(6)</a> <a href='/blog/categories/machine-learning' style='font-size: 150.0%'>machine_learning(5)</a> <a href='/blog/categories/math' style='font-size: 110.0%'>math(1)</a> <a href='/blog/categories/spark' style='font-size: 110.0%'>spark(1)</a> </span>
</section>

<section>
  <h1>访客统计</h1>
    <br/>
    <a href="http://s11.flagcounter.com/more/PbH"><img src="http://s11.flagcounter.com/count2/PbH/bg_FFFFFF/txt_000000/border_CCCCCC/columns_2/maxflags_10/viewers_0/labels_0/pageviews_0/flags_0/percent_0/" alt="Flag Counter" border="0"></a>
</section>    

  
</aside>


    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2018 - Dong Guo -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  


<!--
<div id="fb-root"></div>
<script>(function(d, s, id) {
  var js, fjs = d.getElementsByTagName(s)[0];
  if (d.getElementById(id)) {return;}
  js = d.createElement(s); js.id = id; js.async = true;
  js.src = "//connect.facebook.net/en_US/all.js#appId=212934732101925&xfbml=1";
  fjs.parentNode.insertBefore(js, fjs);
}(document, 'script', 'facebook-jssdk'));</script>

-->
<!--
-->
<!--
-->



</body>
</html>
