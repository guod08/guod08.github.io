
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Dong Guo's Blog</title>
  <meta name="author" content="Dong Guo">

  
  <meta name="description" content="简介 第一次接触EP是10年在百度实习时，当时组里面正有计划把线上的CTR预估模型改成支持增量更新的版本，读到了微软一篇基于baysian的CTR预估模型的文章（见推荐阅读5），文章中没有给出推导的细节，自己也没有继续研究。今年在PRML中读Approximal &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://guod08.github.io">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/atom.xml" rel="alternate" title="Dong Guo's Blog" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="./javascripts/libs/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">

  

</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">Dong Guo's Blog</a></h1>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:guod08.github.io" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div class="blog-index">
  
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/01/01/expectation-propagation/">Expectation Propagation: Theory and Application</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-01-01T22:04:00+08:00" pubdate data-updated="true">Jan 1<span>st</span>, 2014</time>
        
        
           | <a href="/blog/2014/01/01/expectation-propagation/#comments">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><h2>简介</h2>

<p>第一次接触EP是10年在百度实习时，当时组里面正有计划把线上的CTR预估模型改成支持增量更新的版本，读到了微软一篇基于baysian的CTR预估模型的文章（见推荐阅读5），文章中没有给出推导的细节，自己也没有继续研究。今年在PRML中读Approximal inference这章对EP有了一些了解，同时参考了其它相关的一些资料，在这里和大家探讨。</p>

<h4>什么是期望传播</h4>

<p>期望传播(Expectation Propagation): 基于<strong>bayesian</strong>的一种<strong>近似</strong>推断方法，常用于图模型中计算单个节点的边缘分布或者后验分布，属于message passing这一类推断方法。</p>

<h4>牛人</h4>

<p>首先当然是Thomas Minka, 其在MIT读博期间提出了EP，并将EP作为博士论文课题在2001年发表。Minka毕业之后去了CMU教书，现在和Bishop一起在剑桥微软研究院。</p>

<p>其次是Kevin p. Murphy, 他是我做EP相关文献调研时发现的paper比较多的，我读到的一篇全文基本都是在推导Minka博士论文中一些公式的细节。btw Murphy 2013年出版了一本书，见推荐阅读2。</p>

<h4>中英文对照</h4>

<p>下面是一些关键词的中英文对应 （由于相关的书籍文献基本都是英文的，有些词没有想到比较好的中文翻译，故保留英文）</p>

<p>截断高斯: Truncated Gaussian</p>

<p>置信传播: Belief Propagation （后面会简称BP）</p>

<p>期望传播: Expectation Propagation (后面会简称为EP)</p>

<p>消息传递: Message passing</p>

<h2>背景</h2>

<p>EP本身的思想和方法都还是比较简单的，不过会涉及到一些背景知识，这边一并介绍。</p>

<h3>高斯、截断高斯</h3>

<p>EP的核心思想之一是用指数族分布近似复杂分布，实际应用中通常选择高斯分布，所以多个高斯分布的乘积，相除，积分在EP应用过程中不可避免。</p>

<p>截断高斯是高斯分布在指定区间归一化后的结果，（所以其并不是一个高斯分布），EP本身并不和截断高斯直接相关，但是如果在分类问题中应用EP，对观察样本（0-1）建模方法通常是y=sign(f(x)>t), 和另一个高斯分布相乘之后即为截断高斯分布。（然后就需要计算其的均值方差，原因后面会提到）</p>

<p>我在另一篇文章<a href="http://dongguo.me/blog/2013/12/02/gaussian-and-truncated-gaussian/">Gaussian and Truncated Gaussian</a>中介绍了比较多的细节，可以参考。</p>

<h3>指数族分布</h3>

<p>指数族分布（exponential family distribution）有着非常好的特性，比如其有<strong>充分统计量</strong>，多个指数族分布的乘积依然是指数族分布，具体的介绍可以参见wikipedia, 介绍的非常全面，也可以参考PRML第2章。</p>

<p>由于指数族的良好特性，其常被拿去近似复杂的概率分布（EP和variance baysian都是）。由于EP中常常选择高斯分布，我们这边强调一下，高斯分布的充分统计量为: (x, x<sup>2</sup>), 其中x为高斯分布的自变量。</p>

<h3>图模型</h3>

<p>EP是贝叶斯流派的计算变量后验分布（或者说是边缘分布）的近似推断方法，通常都可以通过一个概率图模型来描述问题的生成过程（generation process），所以可以说图模型是EP的典型应用场景。</p>

<p>图模型在很多地方都有介绍，比如PRML第8章，在这里就不重复了。有1点提一下，一个图模型的联合分布（不管是有向图还是无向图）可以写成若干个因子的乘积，对于有向图每个因子是每个节点的条件分布（条件于其的所有直接相连的父节点），对于无限图每个因子是energy function。
这个特性在后面的置信传播算法会用到。</p>

<h3>factor graph</h3>

<p>图模型中节点之间的关系通过边来表达，factor graph将这种节点之间的关系通过显式的节点（factor node）来表达，比如对于有向图，每个factor node就代表一个条件概率分布，图中的所有的信息都存在于节点上（variable nodes和factor nodes）。</p>

<p>后面的BP和EP都基于factor graph，可以认为factor graph使得图上的inference方法变得比较直观，另一个好处是factor graph屏蔽了有向图和无向图的差异。（有向图无向图都可以转变为factor graph）</p>

<p>更多了解可以看PRML第8章。</p>

<h3>置信传播</h3>

<p>Belief Propagation (BP)又叫&#8217;sum-product&#8217;，是一种计算图模型上节点边缘分布的推断方法，属于消息传递方法的一种，非近似方法（基于其延伸的Loopy Belief propagation为近似推断方法）。
BP的核心为如下3点：</p>

<ul>
<li><p><strong>单个variable node边缘分布的计算</strong></p>

<p>  <img class="left" src="/images/personal/research/ep-intro/marginal_dis_variable_node.png" width="400"></p>

<hr />

<p>  前面提到过图模型的联合分布可以分解为若干因子的乘积，每个因子对应一个factor node。</p>

<p>  <img class="left" src="/images/personal/research/ep-intro/posterior_dis_variable_node_f1.png" width="800" height="30"></p>

<p>  每个variable node的边缘分布为与其直接相连的factor nodes传递过来的message的乘积。</p>

<p>  <img class="left" src="/images/personal/research/ep-intro/posterior_dis_variable_node_f2.png" width="800" height="50"></p></li>
<li><p><strong>从factor node到variable node的消息传递</strong>
  <img class="left" src="/images/personal/research/ep-intro/message_factor_to_variable.png" width="400"></p>

<p>  从factor node f传递到variable node x的message为：与f直接相连（除了x）的variable nodes传递到f的messages与f本身的乘积的积分（积分变量为与f直接相连的除x之外的所有variable nodes）</p>

<p>  <img class="left" src="/images/personal/research/ep-intro/message_factor_to_variable_f1.png" width="600"></p></li>
<li><p><strong>从variable node到factor node的消息传递</strong></p>

<p>  <img class="left" src="/images/personal/research/ep-intro/message_variable_to_factor.png" width="400"></p>

<p>  从variable node x到factor node f的message为：与x直接相连的factor nodes（除f意外）传递到x的messages的乘积</p>

<p>  <img class="left" src="/images/personal/research/ep-intro/message_variable_to_factor_f1.png" width="400" height="50"></p></li>
</ul>


<p>更多细节请参考PRML</p>

<h3>Moment matching</h3>

<p>在实际的问题中，要么后验分布本身比较复杂（推荐阅读3中的Clutter example），要么最大化后验的计算比较复杂，要么破坏了具体算法的假设（比如EP要求图中的所有message都是指数族），所以常常会用（有良好性质的）指数族分布近似实际的概率分布。</p>

<p><img class="left" src="/images/personal/research/ep-intro/moment_matching_1.png" width="600">
<img class="left" src="/images/personal/research/ep-intro/moment_matching_2.png" width="600"></p>

<p>用一个分布去近似另一个分布的常见方法是最小化KL散度:
<img class="left" src="/images/personal/research/ep-intro/moment_matching_3.png" width="600">
<img class="left" src="/images/personal/research/ep-intro/moment_matching_4.png" width="600"></p>

<p>我们发现通过最小化KL散度得到的‘最接近’p(x)的q(x)可以简单地通过匹配充分统计量的期望得到。</p>

<p>当q(x)为高斯分布的时候，我们知道其充分统计量u(x)=(x, x<sup>2</sup>)，这时通过KL散度最小化近似分布近似的方法称为moment matching(匹配矩)
<img class="left" src="/images/personal/research/ep-intro/moment_matching_5.png" width="600"></p>

<p>为什么称为匹配矩呢，看看矩的定义就知道了：
<img class="left" src="/images/personal/research/ep-intro/moment_matching_6.png" width="600"></p>

<h2>期望传播方法-理论</h2>

<p>EP的思想：在图模型中，用高斯分布近似每一个factors，然后&#8217;approximate each factor in turn in the context of all remaining facotrs&#8217;.</p>

<p>下面为具体的算法：
<img class="left" src="/images/personal/research/ep-intro/ep_def.png" width="800"></p>

<p>下面通过Minka博士论文中的例子‘clutter problem’来解释：
每个观察样本以(1-w)的概率由高斯分布N(x|sita, I)生成，以w的概率由noise生成（同样也是高斯分布N(x|0, aI)），于是：
<img class="left" src="/images/personal/research/ep-intro/clutter_problem_1.png" width="400">
<img class="left" src="/images/personal/research/ep-intro/clutter_problem_2.png" width="200"></p>

<p>按照EP的思想，我们用一个单高斯q(sita)去近似混合高斯p(x|sita)
<img class="left" src="/images/personal/research/ep-intro/clutter_problem_3.png" width="400"></p>

<p>单高斯去近似混合高斯听起来效果一定不好，但实际上，由于EP在近似的时候乘了其他所有factors的高斯近似之后的上下文，考虑到很多个高斯分布相乘之后的方差一般都很小，所有实际上单高斯只需要在很小的区间近似好混合高斯即可。如下图：
<img class="left" src="/images/personal/research/ep-intro/clutter_problem_4.png" width="400">
<img class="left" src="/images/personal/research/ep-intro/clutter_problem_5.png" width="400">
其中蓝色曲线为混合高斯（没有画完整），红色曲线为近似的单高斯，绿色曲线为‘其它所有factor的乘积’。</p>

<h3>EP怎么应用在message passing中：</h3>

<p>在图模型中，所谓的&#8217;context of all remaining factors&#8217;就是当前节点之外所有节点和messages，所以EP在图模型中的使用方式为：和BP一样的方法计算message和marginal distribution，当某个factor或者marginal distribution不是高斯分布时，用高斯分布近似它。所以Minka认为EP也就是BP+moment matching。</p>

<p>由于每个factor以及variable node的边缘分布都是高斯分布（或被近似为高斯分布），所以EP的计算过程一般并不复杂。</p>

<h2>期望传播方法-应用</h2>

<p>EP被广泛地应用在图模型的inference上，这边提一下微软的2个应用：Bing的CTR预估，XBOX游戏中player skill的评估。</p>

<h3>Bing的CTR预估</h3>

<p>详细的推导及实验请参考：<a href="http://dongguo.me/blog/2013/12/01/bayesian-ctr-prediction-for-bing/">Bayesian CTR prediction for Bing</a>
paper中称这个model为ad predictor，其在我的数据集上预估效果很不错，训练预测速度快，天然支持增量更新，主要的缺点就是模型不是稀疏的。如果你知道怎么自然地达到稀疏效果，请指教。</p>

<p>和其它算法的比较请参考：<a href="http://dongguo.me/blog/2013/12/15/classification-models/">Classification Models</a></p>

<h3>XBOX中player skill的评估</h3>

<p>图模型和上一篇略有差异，推导过程差不多，paper中没有给出详细的推导过程，不过Murphy的新书中给出了，请参考推荐阅读2。</p>

<h2>一些小结</h2>

<ol>
<li>EP的通用性比较好，对于实际的问题，画出graph model和factor graph，就可以尝试用EP来进行inference；</li>
<li>虽然应用EP时的推导过程略长（计算很多个message和marginal distribution），但是最终的整体的更新公式一般都非常简单，所以模型训练时间开销往往较小；</li>
<li>为了使用EP，只能用高斯分布来建模，比如Bing的CTR预估那篇对每个feature的weight建模，只能假设服从高斯分布，相当于是2范数的正则化，不能达到稀疏模型的效果；</li>
<li>在我的实验中，通过EP进行inference得到的模型预估效果不错，值得一试；</li>
</ol>


<h2>推荐阅读</h2>

<ol>
<li><p>机器学习保留书籍:<a href="http://research.microsoft.com/en-us/um/people/cmbishop/prml/">Pattern recognition and machine learning</a> 第2,8,10章 (第2章看看高斯四则运算，指数族分布特性；第8章了解图模型基础，期望传播算法；第10章了解期望传播算法)</p></li>
<li><p>Murphy新书: <a href="http://www.cs.ubc.ca/~murphyk/MLbook/">Machine Learning: A Probabilistic Perspective</a> 第22章 (本书相比PRML更加具体，第22章干脆包含了TrueSkill的详细推导步骤)</p></li>
<li><p>Minka的博士论文：<a href="http://qh.eng.ua.edu/e_paper/e_thesis/EPThesis.pdf">A family of algorithms for approximate Bayesian inference</a> (想了解基本思想和理论看完前3节即可)</p></li>
<li><p>EP的应用之一：<a href="http://research.microsoft.com/pubs/74419/tr-2006-80.pdf">TrueSkill: A Bayesian Skill Rating System</a> (文中并没有给出EP每一步的细节)</p></li>
<li><p>EP的应用之二：<a href="http://research.microsoft.com/pubs/122779/adpredictor%20icml%202010%20-%20final.pdf">Web-Scale Bayesian Click-Through Rate Prediction for Sponsored Search Advertising in Microsoft’s Bing Search Engine</a> (CTR预估的应用比较吸引人，文章写得很棒，算法的效果也很好，只是干脆忽略的inference过程，有兴趣的同学可以参看我另一个文章，里面有一步一步推导的过程)</p></li>
<li><p>Minka整理的EP学习资料：<a href="http://research.microsoft.com/en-us/um/people/minka/papers/ep/roadmap.html">link</a> (其中的包含了一个videolecture上他做的variance inference的talk值得一看)</p></li>
</ol>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2013/12/16/some-ML-slides-from-Hulu-internal/">Some ML Slides From Hulu Internal</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-12-16T00:00:00+08:00" pubdate data-updated="true">Dec 16<span>th</span>, 2013</time>
        
        
           | <a href="/blog/2013/12/16/some-ML-slides-from-Hulu-internal/#comments">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p><a href="http://www.slideshare.net/guo_dong/machine-learning-introduction">Machine Learning Introduction</a></p>

<p><a href="http://www.slideshare.net/guo_dong/feature-selection">Feature selection</a></p>

<p><a href="http://www.slideshare.net/guo_dong/logistic-regressionpptx">Logistic regression</a></p>

<p><a href="http://www.slideshare.net/guo_dong/additive-model-and-boosting-tree">Additive Model and boosting tree</a></p>

<p><a href="http://www.slideshare.net/guo_dong/expectation-propagation-researchworkshop">Expectation propagation</a> (one popular bayesian inference technique proposed by Thomas Minka)</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2013/12/15/classification-models/">Classification Models</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-12-15T00:35:00+08:00" pubdate data-updated="true">Dec 15<span>th</span>, 2013</time>
        
        
           | <a href="/blog/2013/12/15/classification-models/#comments">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>During my past 3 years in career, following classifiers are often used for classification tasks.</p>

<h2>Typcial classifiers comparision</h2>

<p><img class="left" src="/images/personal/research/classifiers/classifiers_compare.png" width="800"></p>

<h2>Decision Tree</h2>

<p>Decision Tree is not a start-of-art model for classification or regression, and when there are huge features(say millions) it will take a long time for training.
But it may perform very well when the number of distinct features are limited, and the classification/regression task is obviously non-linear.</p>

<p>A typical scenario is multi-model fusion: you have trained multiple models for single task, and you want to generate the final prediction result using all these models.
Based on my past experiments, Decision Tree can out perform linear model(linear regression, logistic regression and so on) on many datasets.</p>

<h2>RDT, random forest, boosting tree</h2>

<p>All of these 3 models are ensemble learning method for classification/regression that operate by constructing multiple Decision Tree at training time.
For RDT(random decision tree), only part of total samples are used to training each tree. And all features are considered for splitting.</p>

<p>Similar with RDT, random forest also use part of total sampels to construct each tree, but it also only use subset of features/dimisions for splitting.
So random forest introduces more &lsquo;random&rsquo; factors for training, and it may perform better when there are more noises in training set.</p>

<p>boosting tree is actually forward stagwise additive modeling with decision tree as base learner. And if you choose exponential loss function, then boosting tree becauses Adaboost with decision tree as base learner.
Here is one <a href="http://www.slideshare.net/guo_dong/additive-model-and-boosting-tree">slide</a> about additive model and boosting tree.</p>

<h2>Generalized linear model</h2>

<p>One of the most popular generalized linear model is logistic regression, which is generalized linear model with inversed sigmoid function as the link function.
There are multiple different implementation for logistic regression, and here are some often used by me.</p>

<h4>Logistic regression optimized with SGD.</h4>

<p>It&rsquo;s very basic, so I ignore the details here</p>

<h4>OWLQN</h4>

<p>It was proposed by Microsoft in paper <a href="http://research.microsoft.com/en-us/downloads/b1eb1016-1738-4bd5-83a9-370c9d498a03/">Orthant-Wise Limited-memory Quasi-Newton Optimizer for L1-regularized Objectives</a> of ICML 2007. You can also find the source code and executable runner via this link.</p>

<p>This model is optimized by a method which is similar with L-BFGS, but can achieve sparse model with L1 regularizer. I recommend you try this model and compare with other models you are using in your dataset.
Here are four reasons:</p>

<ol type="a">
<li>It&rsquo;s fast, especially when the dataset is huge;</li>
<li>It can generate start-of-art prediction results on most dataset;</li>
<li>It&rsquo;s stable and there are few parameters need to be tried. Actaully, I find only regularization parameters can impact the performance obviously;</li>
<li>It&rsquo;s sparse, which is very important for big dataset and real product. (Of course, sparse is due to L1 regularizer, instead of the specific optimization method)</li>
</ol>


<p>One problem is it&rsquo;s more challenge to implement it by yourself, so you need spend some time to make it support incremental update or online learning.</p>

<h4>FTRL</h4>

<p>It was proposed by Google via paper <a href="http://static.googleusercontent.com/media/research.google.com/en/us/pubs/archive/41159.pdf">
Ad Click Prediction: a View from the Trenches</a> in 2013. I tried on my dataset, and this implementation can generate similar prediction performance with OWLQN.
It&rsquo;s quicker than OWLQN for training, and it&rsquo;s also sparse. One advantage is it&rsquo;s very easy to implement, and it support increamental update naturally.
One pain point for me is this model has 3-4 parameters need to be chosen, and most of them impact the prediction performance obviously.</p>

<h4>Ad predictor</h4>

<p>This <a href="http://research.microsoft.com/pubs/122779/adpredictor%20icml%202010%20-%20final.pdf">paper</a> was also proposed by Microsoft in ICML 2009.</p>

<p>One biggest different with upper 3 implementation is it&rsquo;s based on bayesian, so it&rsquo;s generative model. Ad predictor is used to predict CTR of sponsor search ads of Bing, and on my dataset, it could also achieve comparable prediction performance with OWQLN and FTRL.
Ad predictor model the weight of each feature with a gaussian distribution, so it natually supports online learning. And the prediction result for each sample is also a gaussian distribution, and it could be used to handle the exploration and exploitation problem.
See more details of this model in another <a href="http://guod08.github.io/me/blog/2013/12/01/bayesian-ctr-prediction-for-bing/">post</a>.</p>

<h2>Neural Network</h2>

<p>ANN is so slow for training, so it&rsquo;s tried only when the dataset is small of medium. Another disadvantage of ANN is it&rsquo;s totally blackbox.</p>

<h2>SVM</h2>

<p>SVM with kernel is also slow for training. You can try it with <a href="http://www.csie.ntu.edu.tw/~cjlin/libsvm/">libsvm</a>.</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2013/12/02/gaussian-and-truncated-gaussian/">Gaussian and Truncated Gaussian</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-12-02T18:30:00+08:00" pubdate data-updated="true">Dec 2<span>nd</span>, 2013</time>
        
        
           | <a href="/blog/2013/12/02/gaussian-and-truncated-gaussian/#comments">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>Everybody knows about Gaussian distribution, and Gaussian is very popular in Bayesian world and even in our life. This article summaries typical operation of Gaussian, and something about Truncated Guassian distribution.</p>

<h2>Gaussian</h2>

<h3>pdf and cdf</h3>

<p><img class="left" src="/images/personal/research/gaussian/gaussian_pdf.png" width="350" title="'Gaussian pdf'" >
<img class="right" src="/images/personal/research/gaussian/gaussian_cdf.png" width="350" title="'Gaussian cdf'" >
<img src="/images/personal/research/gaussian/gaussian_1.png" width="1200"></p>

<h3>Sum/substraction of two independent Gaussian random variables</h3>

<p><img src="/images/personal/research/gaussian/gaussian_plus2.png" width="1200">
Please take care upper formula only works when x1 and x2 are independent. And it&rsquo;s easy to get the distribution for variable x=x1-x2
See <a href="http://www.dartmouth.edu/~chance/teaching_aids/books_articles/probability_book/Chapter7.pdf">here</a> for the detils of inference</p>

<h3>Product of two Gaussian pdf</h3>

<p><img src="/images/personal/research/gaussian/gaussian_multiple2.png" width="1200">
Please take care x is no longer a gaussian distribution. And you can find it&rsquo;s very elegant to use &lsquo;precision&rsquo; and &lsquo;precision adjusted mean&rsquo; for Gaussian operation like multiply and division.
See <a href="http://www.tina-vision.net/docs/memos/2003-003.pdf">here</a> for the detils of inference</p>

<h3>Division of two Gaussian pdf</h3>

<p><img src="/images/personal/research/gaussian/gaussian_divide2.png" width="1200"></p>

<h3>Intergral of the product of two gaussian distribution</h3>

<p><img src="/images/personal/research/gaussian/gaussian_integral.png" width="1200"></p>

<h2>Truncated Gaussian</h2>

<p><img class="left" src="/images/personal/research/gaussian/tg_pdf.png" width="350" title="'Gaussian pdf'" >
<img class="right" src="/images/personal/research/gaussian/tg_cdf.png" width="350" title="'Gaussian cdf'" ></p>

<p>Truncated Gaussian distribution is very simple: it&rsquo;s just one conditional (Gaussian) distribution.
Suppose variable x belongs to Gaussian distribution, then x conditional on x belongs to (a, b) has a truncated Gaussian distribution.
<img src="/images/personal/research/gaussian/tg_def.png" width="1200"></p>

<h3>Calculate expectation of Truncated Gaussian</h3>

<p><img src="/images/personal/research/gaussian/tg_e1.png" width="1200">
<img src="/images/personal/research/gaussian/tg_e2.png" width="1200"></p>

<h3>Calculate variance of Truncated Gaussian</h3>

<p><img src="/images/personal/research/gaussian/tg_v1.png" width="1200">
<img src="/images/personal/research/gaussian/tg_v2.png" width="1200">
<img src="/images/personal/research/gaussian/tg_v3.png" width="1200"></p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2013/12/01/bayesian-ctr-prediction-for-bing/">Bayesian CTR Prediction of Bing</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-12-01T22:07:00+08:00" pubdate data-updated="true">Dec 1<span>st</span>, 2013</time>
        
        
           | <a href="/blog/2013/12/01/bayesian-ctr-prediction-for-bing/#comments">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>Microsoft published a paper in ICML 2009 named &lsquo;Web-Scale Bayesian Click-Through Rate Prediction for Sponsored Search Advertising in Microsoft&rsquo;s Bing Search Engine&rsquo;,
which is claimed won the competition of most accurate and scalable CTR predictor across MS.
This article shows how to inference this model(let&rsquo;s call it Ad predictor) step-by-step.</p>

<h2>Pros. and Cons.</h2>

<p>I like it because it&rsquo;s totally based on Bayesian, and Bayesian is beautiful. Online learning is naturally supported, and the precition accuracy is comparable with FTRL and OWLQN. And both training and prediction is light-weight and fast.
Btw: one shortage of this model is it&rsquo;s not sparse, which may be a big issue when applied on big dataset with huge amount of features.</p>

<h2>Inference using Expectation Propagation step by step</h2>

<p>Firstly, following is the factor graph of ad predictor.
<img class="left" src="/images/personal/research/ep/adPredictor.png" width="1200"></p>

<p><img class="left" src="/images/personal/research/ep/ep_factor_definition.png" width="1200"></p>

<p><img class="left" src="/images/personal/research/ep/ep_step1.png" width="1200"></p>

<p><img class="left" src="/images/personal/research/ep/ep_step2.png" width="1200"></p>

<p><img class="left" src="/images/personal/research/ep/ep_step3.png" width="1200"></p>

<p><img class="left" src="/images/personal/research/ep/ep_step4.png" width="1200"></p>

<p><img class="left" src="/images/personal/research/ep/ep_step5.png" width="1200"></p>

<p><img class="left" src="/images/personal/research/ep/ep_step6.png" width="1200"></p>

<p><img class="left" src="/images/personal/research/ep/ep_step7_1.png" width="1200"></p>

<p><img class="left" src="/images/personal/research/ep/ep_step7_2.png" width="1200"></p>

<p><img class="left" src="/images/personal/research/ep/ep_step7_3.png" width="1200"></p>

<p><img class="left" src="/images/personal/research/ep/ep_step7_4.png" width="1200"></p>

<p><img class="left" src="/images/personal/research/ep/ep_step8.png" width="1200"></p>

<p><img class="left" src="/images/personal/research/ep/ep_step9.png" width="1200"></p>

<p><img class="left" src="/images/personal/research/ep/ep_step10.png" width="1200"></p>

<p><img class="left" src="/images/personal/research/ep/ep_step11.png" width="1200"></p>

<p><img class="left" src="/images/personal/research/ep/ep_step12.png" width="1200"></p>

<p><img class="left" src="/images/personal/research/ep/ep_step13.png" width="1200"></p>

<p>For each sample, we can use the formula of step 13 to update the posterior parameter of W, which is very easy to be implemented.</p>

<h2>Prediction</h2>

<p>After training, we can predict with following formula:
<img class="left" src="/images/personal/research/ep/ep_predict.png" width="1200"></p>

<h2>Prediction Accuracy</h2>

<p>I compared it with FTRL and OWLQN on one dataset for age&amp;gender prediction. AUC of this model is comparable with OWLQN and FTRL, so I recommend you have a try in your case.</p>

<h2>Insights</h2>

<ol>
<li><p>You can find variance of each feature increases after every exposure, which makes sense.</p></li>
<li><p>This model shows samples with more features will have bigger variance, which does not make sense very much. I think the reason is we assume all the features are independent. Any insights from you?</p></li>
</ol>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2013/10/20/shi-jian-guan-li/">更高效地工作学习</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-10-20T20:53:00+08:00" pubdate data-updated="true">Oct 20<span>th</span>, 2013</time>
        
        
           | <a href="/blog/2013/10/20/shi-jian-guan-li/#comments">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>周末浏览了一本书《小强升职记》，介绍了工作中事情管理的很多经验，个人觉得帮助挺大，在这边分享一下。</p>

<ol>
<li><p>将事情按照重要性、紧急性2个维度分成4个象限。</p>

<ol type="a">
<li>第一象限（重要且紧急）：立马去做；思考“这些事情真的都是重要且紧急的吗？”，“为什么这些事情会进入这个象限？”；</li>
<li>第二象限（重要但不紧急）：尽量开始去做，没时间的话第一时间进行任务分解，制定时间表；</li>
<li>第三象限（不重要但紧急）：注意紧急和重要一点关系没有；思考“如何尽力减少这类事情？”</li>
<li>第四象限（不重要且不紧急）：尽量别做</li>
</ol>
</li>
<li><p>走出第三象限（不重要但紧急）：Monkey theory
 甩掉自己身上的猴子，将猴子放回到主人身上 （书中这段举的例子很有意思）
 比如：“朋友问：这周六咱们去游泳吧；你答：好啊，到时候你提前给我打电话，有时间的话，我一定去”。
 比如“新来的同事问：你好，能不能给我讲讲pre-demo这个项目；你答：好啊，能不能你先看一下咱们wiki上的文档，你总结问题之后咱们一起讨论？”
 另外，第三象限中的事情可以想办法delegate给别人去做</p></li>
<li><p>第二象限是精力分配的重点
 有2个原因：这些事是重要的；这些事安排处理好了，进入第一象限的事情也会明显变少；
 目标描述和任务分解：让自己清晰地自己有哪些细分的是要做，每个时段的任务是什么，明确地知道该任务是否拖延；有利于减轻自己的压力；</p></li>
<li><p>时段工作法：将待做的事情分配到各个小时，每个小时或者任务完成后勾掉任务，给自己增加压力，督促自己提高效率；
 这种方法还可以让自己了解自己的高效、低效时段在哪</p></li>
<li><p>每周（天）开始前将要做的事情分到4个象限; 优先做（或者在自己效率最高的时段）做第一第二象限的事情；同时按照时段工作法进行</p></li>
<li><p>一些tips
 尽量午休
 做一件事情的时候尽量不让自己给打断（如果同事来讨论问题，可以说一会去找他，同时记录下这件事情），专注心如止水地做事
 关掉outlook的邮件提醒，绝大部分邮件都不需要立即响应，每半天主动检查一次邮件即可；
 安排计划前提前考虑已经被预约掉的时间；
 保持办公环境，特别是办公桌的整洁有序；
 刚到公司或者吃完饭回到公司，先给水杯打满水再工作；</p></li>
</ol>

</div>
  
  


    </article>
  
  <div class="pagination">
    
    <a href="/blog/archives">Blog Archives</a>
    
  </div>
</div>
<aside class="sidebar">
  
    <section>
  <h1>Dong Guo</h1>
  <p>Now reach on advertising and machine learning at Hulu (User modeling, ad targeting, ad serving, and inventory projection/management). Previously at Baidu. Graduate from Tsinghua.
  <br></br>
  Email: guod08[at]gmail.com
  Other: <a href="http://www.linkedin.com/in/dongguo1">Linkedin</a>, <a href="https://github.com/guod08/">Github</a></p>
</section>
<section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2014/01/01/expectation-propagation/">Expectation Propagation: Theory and Application</a>
      </li>
    
      <li class="post">
        <a href="/blog/2013/12/16/some-ML-slides-from-Hulu-internal/">Some ML Slides From Hulu Internal</a>
      </li>
    
      <li class="post">
        <a href="/blog/2013/12/15/classification-models/">Classification Models</a>
      </li>
    
      <li class="post">
        <a href="/blog/2013/12/02/gaussian-and-truncated-gaussian/">Gaussian and Truncated Gaussian</a>
      </li>
    
      <li class="post">
        <a href="/blog/2013/12/01/bayesian-ctr-prediction-for-bing/">Bayesian CTR Prediction of Bing</a>
      </li>
    
      <li class="post">
        <a href="/blog/2013/10/20/shi-jian-guan-li/">更高效地工作学习</a>
      </li>
    
  </ul>
</section>


  
</aside>

    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2014 - Dong Guo -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  



<div id="fb-root"></div>
<script>(function(d, s, id) {
  var js, fjs = d.getElementsByTagName(s)[0];
  if (d.getElementById(id)) {return;}
  js = d.createElement(s); js.id = id; js.async = true;
  js.src = "//connect.facebook.net/en_US/all.js#appId=212934732101925&xfbml=1";
  fjs.parentNode.insertBefore(js, fjs);
}(document, 'script', 'facebook-jssdk'));</script>









</body>
</html>
